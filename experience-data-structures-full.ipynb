{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "214979f2",
   "metadata": {},
   "source": [
    "# 3. Loading and Handling Pandas Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6363cd1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Questions\n",
    "\n",
    "* How are Pandas data structures setup?\n",
    "* How to load data into Pandas?\n",
    "* How to write data from Pandas to a file.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "* Understand the usefulness of Pandas when loading data.\n",
    "* Understand how to load data and deal with common issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3c981",
   "metadata": {},
   "source": [
    "## Pandas Data Structures\n",
    "Here, you should follow along inside the Jupyter notebook associated with this lesson. \n",
    "\n",
    "Before we start loading data into Pandas we need to become familiar with 2 of the primary data structures of Pandas:\n",
    "\n",
    " * Series\n",
    " * DataFrames\n",
    " \n",
    "The reason we use Series and DataFrames rather than native python data structures to hold our data is because there are additional attributes and methods associated with Series and DataFrames that will be useful for wrangling and analytics. One of the primary benefits of Series and DataFrames over native python data structures is that it is a very natural way to describe a data set in an excel-like manner by referencing the rows and columns of our data with labels of our choosing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9c37b",
   "metadata": {},
   "source": [
    "### Pandas Series Vs. DataFrames\n",
    "\n",
    "* Pandas has two principal data structures, Series and DataFrames. If you are familiar with Microsoft’s Excel application then you can liken Series to single columns (or rows) in an Excel sheet and DataFrames to entire tables (or spreadsheets).\n",
    "\n",
    "![](https://change-hi.github.io/morea/data-wrangling/fig/E3_1_series_vs_dataframe.png)\n",
    "\n",
    "We see in the image above that a Series in the context of Excel could be the first row of the spreadsheet, while a DataFrames would be the entire spreadsheet. In other words, a DataFrames is simply a collection of labeled Series (those are rows or columns).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ab8134",
   "metadata": {},
   "source": [
    "## Pandas Data Types\n",
    "\n",
    "Pandas attempts to assign the correct type to each column based on the type of data it contains. The mapping from the native python types to what they would be in Pandas is summarized below.\n",
    "\n",
    "![](https://www.dropbox.com/s/ir2x9gccdgeomq7/pandas_data_types.png?dl=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14d6802",
   "metadata": {},
   "source": [
    "## Pandas DataFrames Basics\n",
    "\n",
    "DataFrames are essentially ordered collections of Series with two associated Index objects, one to label rows and another to label columns. Reiterating the example mentioned earlier, it helps to think of DataFrames as MS. Excel spreadsheets where each row (or column) as an individual Series.\n",
    "\n",
    "You can create DataFrames from either loading in a file e.g. a csv file or by converting it from a native Python data structure. However, here we will be focusing on loading data from a file and turning it into a DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b4b2b1",
   "metadata": {},
   "source": [
    "## Import the Pandas Package\n",
    "\n",
    "Before we can load in data from a file we need to load the pandas package. Pandas is often imported with the shortcut, or alias, `pd` to reduce the amount of characters needed to use the different methods within it.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ea14b0",
   "metadata": {},
   "source": [
    "## Loading and Parsing Data\n",
    "\n",
    "Pandas can load in data from a variety of file formats. In most cases you will be loading in data from a plain text file since this is one of the most common and simple ways to store data. Plain text files only contain text without any special formatting or code e.g. .txt, .csv, or .tsv files.\n",
    "\n",
    "To store data in a plain text file you need a standard way of distinguishing the individual data entries. For example, suppose a file contained the following text:\n",
    "\n",
    "```bash\n",
    ",column1,column2,column3,\n",
    "row1,a,b,c\n",
    "row2,d,e,f\n",
    "row3,g,h,i\n",
    "```\n",
    "\n",
    "A human would see the text above and may be able to discern the 3 columns and 3 rows and the individual data entries and see that the file contains a table that looks like the one below.\n",
    "```bash\n",
    "|      | column 1 | column 2 | column 3 |\n",
    "| ---- | -------- | -------- | -------- |\n",
    "| row1 | a        | b        | c        |\n",
    "| row2 | d        | e        | f        |\n",
    "| row3 | g        | h        | i        |\n",
    "\n",
    "```\n",
    "\n",
    "However, a computer would have no idea how to parse this without any direction; to a computer the text above is just one string of characters. To help the computer parse the text, we could tell it that each data entry is separated by a column and each row is separate by a new line. This way of organizing data is called a plain text file format.\n",
    "\n",
    "Most of the plain text file formats fall into one of the following two categories: Delimited and Fixed Width\n",
    "\n",
    "* Delimitted files are organized such that columns and rows are seperated by a certain character called a delimiter\n",
    "* Fixed width files are those where each entry in a column has a fixed number of characters\n",
    "\n",
    "We will focus on the delimited `.csv` file type. The acronym csv stands for ‘Comma Separated Values’ and informs us that the column entries are delimited by commas and the rows are delimited by a new line. So, the example we discussed in this cell conforms to the `.csv` format.\n",
    "\n",
    "Included in the Pandas toolkit is a collection of parsing functions to read and build `DataFrames` from data that is stored in a variety of formats. The Pandas function which parses and builds a `DataFrame` from a generic delimited plain text file is read_csv(). To run read_csv() with the default settings we only need to provide one positional argument, the path to the file we want to read. If we wanted to read a file, say `my_data.csv` in the same directory as our notebook  and put the data into a new Pandas DataFrame, `df`, we would type: \n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"my_data.csv\")\n",
    "```\n",
    "\n",
    "However, parsing plain text files can become a complicated procedure. To aid in the process, Pandas provides a lot of optional parameters that may be set when calling the `read_csv()` function. To learn more, see the `read_csv()` documentation, which summarizes all of the parameters. We will be covering many of the most important parameters throughout the rest of this lesson.\n",
    "\n",
    "By default `read_csv()` will separate data entries when it encounters a comma and will separate rows by new lines encoded by `'\\n'`. If we wanted to change this behavior so that `read_csv()` separates by tabs (encoded with `'\\t'`), then we can set the optional parameter `sep = '\\t'`. For instance, if we wanted to read the data in the file ‘tsv_example.tsv’, which is a tab separated values file, and save the data in a Pandas DataFrame called df, then we would type:\n",
    "\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('data/tsv_example.tsv', sep='\\t')\n",
    "```\n",
    "\n",
    "Both methods will lead to an equivalent DataFrame that looks like the following:\n",
    "\n",
    "\n",
    "![](https://change-hi.github.io/morea/data-wrangling/fig/E3_2_loaded_dataframe.png)\n",
    "\n",
    "\n",
    "In the previous examples we loaded the entire dataset from the file we gave Pandas. However, when working with large datasets it is good practice to load your data in small pieces before loading the entire dataset to ensure that the file is parsed correctly. Small data sets are more manageable and errors are easier to spot, while large data sets take more time to parse. So, a good workflow is to read a small portion of the data and analyze the resulting data frame to see if you need to modify any of the default behaviors of the read function.\n",
    "\n",
    "To load only up to a limited number of rows we can use the `nrows` parameter for both `read_table()` and `read_csv()`. For example, the file `E3_tara_w1.csv` is a csv file with over 200 rows, but if we wanted to read only the first 5 rows of this file we can call the Pandas `read_csv()` function and set `nrows = 5`:\n",
    "\n",
    "##### INPUT\n",
    "```python\n",
    "\n",
    "df = pd.read_csv('data/tsv_example.tsv', nrows=5)\n",
    "df.shape # Returns the number of rows and columns of the DataFrame 'df' (rows, columns)\n",
    "```\n",
    "##### OUTPUT\n",
    "```bash\n",
    "(5, 7)\n",
    "```\n",
    "\n",
    "We see that the DataFrame `df`, that we saved the data in, has a shape attribute of (5, 7). This means that there are 5 rows (since we set nrows=5) and 7 columns (all the columns of the dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba5a62d",
   "metadata": {},
   "source": [
    "## Headers and Indexes\n",
    "\n",
    "When we loaded the previous datasets `read_csv()` assumed that the first row in our `.csv` file contained a header, or labels for each of the columns. If we want to load in a dataset that does not contain a header row we can tell `read_csv()` that there is no header by setting `header=None`.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"data/noheader_example.csv\", header=None)\n",
    "df\n",
    "```\n",
    "\n",
    "\n",
    "However, this does not mean that the DataFrame does not have headers but rather that Pandas will set them to be an integer value. An example is shown in the figure below:\n",
    "\n",
    "![](https://change-hi.github.io/morea/data-wrangling/fig/E3_3_no_columns_dataframe.png)\n",
    "\n",
    "You might also notice that there is also a corresponding integer number in the left side of each row. This is the index that is essentially the label, or “name”, for each row. If we have a column that is specifies each row in the Python file we can tell Pandas to use that column instead of the default of using a integer. This can be done by e.g. setting `index_col='col_name'` however, if you don’t have any headers you can also specify the column by using its integer location e.g. `index_col=0`. Note that the integer location of a column goes from left to right and starts at 0.\n",
    "\n",
    "```python\n",
    "df = pd.read_csv('data/noheader_example.csv', header=None, index_col=0)\n",
    "df\n",
    "```\n",
    "\n",
    "![](https://change-hi.github.io/morea/data-wrangling/fig/E3_4_no_column_index_specified_dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ff33e",
   "metadata": {},
   "source": [
    "## Common Data Loading Problems\n",
    "\n",
    "Though data storage in plain text files should follow certain formats like `.csv` for ‘Comma Separated Values’ and `.tsv` for ‘Tab Separated Values’, there is still some ambiguity on how things like missing entries, and comments should be denoted. For this reason Pandas has implemented functionality into the `read_table()` and `read_csv()` to help ‘clean’ plain text data.\n",
    "\n",
    "Below we will go through two examples of common issues when loading in data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a148a0",
   "metadata": {},
   "source": [
    "### Missing Values\n",
    "\n",
    "There are often missing values in a real-world data set. These missing entries may be identifiable in the dataset by a number of different tags, like ‘NA’, ‘N.A.’, ‘na’, ‘missing’, etc. It is important to properly identify missing values when creating a DataFrame since certain DataFrame methods rely on the missing values being accounted for. For example, the `count()` method of a DataFrame returns the number of non-missing values in each column. If we want this to be an accurate count, then we need to be sure of pointing out all the tags which represent ‘missing’ to Pandas.\n",
    "\n",
    "For example, if we were to load in a `.csv` where missing values are ‘Null’ and not specify this then Pandas will load these values in as objects. To let Pandas know that we want to interpret these values as missing values we can add `na_values='Null'`.\n",
    "\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(\"data/null_values_example.csv\", na_values='Null')\n",
    "df\n",
    "```\n",
    "\n",
    "Without `na_values='Null'`:\n",
    "    \n",
    "![](https://change-hi.github.io/morea/data-wrangling/fig/E3_5_null_values.png)\n",
    "\n",
    "With `na_values='Null'`:\n",
    "    \n",
    "![](https://change-hi.github.io/morea/data-wrangling/fig/E3_6_nan_values.png)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a217ff1",
   "metadata": {},
   "source": [
    "### Auto NaN values\n",
    "\n",
    "Pandas will interpret certain values as being `NaN` values even without user Python. For example if ‘NULL’ is found then Pandas will treat it as a missing value and treat it as a `NaN` value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e8c47",
   "metadata": {},
   "source": [
    "## Writing Data in Text Format\n",
    "\n",
    "Now that we are familiar with the reading mechanisms that Pandas has implemented for us, writing DataFrames to text files follows naturally. Pandas DataFrames have a collection of `to_<filetype>` methods we can call; we will focus on `to_csv()`. For exmaple, `to_csv()` takes the parameter path (the name and location of the file you are writing) and will either create a new file or overwrite the existing file with the same name.\n",
    "\n",
    "The fucntion `to_csv()` has a number of optional parameters that you may find useful, all of which can be found in the Pandas documentation.\n",
    "\n",
    "```python\n",
    "df.to_csv('data/new_file.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319b6a8f",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "\n",
    "* Pandas provides numerous attributes and methods that are useful for wrangling and analyzing data.\n",
    "* Pandas contains numerous methods to help load/write data to/from files of different types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d81f1a-5502-4c5d-b8fb-549bcee466f6",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Try it yourself! Load the first 10 lines of the excel file '20_sales_records.xlsx' into a variable and then display your DataFrame. Identify the number of rows and columns of your resulting DataFrame.\n",
    "\n",
    "* The file is located in the `data` folder.\n",
    "* Use the `read_excel` command along with the argument you learned to parse a specified number of rows.\n",
    "* This file has `NaN` values that are not automatically detected. They are labeled as `'none'`. Have Pandas interpret these as `NaN` values upon loading of the dataset.\n",
    "* Display the results.\n",
    "* Identify the number of rows and columns of your resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d8bf4-bdd7-41fd-bac0-adc57969de3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
